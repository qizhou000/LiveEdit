edit_model_name: "minigpt-4-vicuna-7b"
edit_layer_inpt_path: "llama_model.model.layers.30.mlp"
edit_layer_outpt_path: "llama_model.model.layers.30.mlp"
edit_layer_i_of_inpt: 18
llm_layer_tmp: "llama_model.model.layers"
llm_hidden_dim1: 4096
llm_hidden_dim2: 11008
lora_rank: 32
lora_edit_batch_size: 20
max_steps: 100
min_loss: 0.03
lr: 1.e-4
topk: 1